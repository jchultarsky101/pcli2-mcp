# MCPHost Configuration for Local Ollama Connection

# MCP Servers - Built-in tools that can be used with your LLM
mcpServers:
  # Filesystem server for file operations
  filesystem:
    type: "builtin"
    name: "fs"
    options:
      allowed_directories: ["/Users/julian", "/tmp"]

  # Bash server for executing shell commands
  bash:
    type: "builtin"
    name: "bash"

  # HTTP server for making web requests
  http:
    type: "builtin"
    name: "http"

  # PCLI2 MCP server for Physna operations
  pcli2-mcp:
    type: "local"
    command: ["/Users/julian/projects/physna/pcli2-mcp/target/release/pcli2-mcp"]
    # Note: No --stdio flag needed as mcphost handles stdio transport automatically

# Model configuration
# model: "ollama:qwen3:8b"  # Using your Qwen3 model
# model: "ollama:llama3.2:1b"
model: "ollama:mistral:7b"

# Maximum steps for the model to take
max-steps: 20

# Debug mode (set to true for verbose output)
debug: false

# Model generation parameters
max-tokens: 4096
temperature: 0.7
top-p: 0.95
top-k: 40
stop-sequences: ["Human:", "Assistant:"]

# Provider settings for Ollama
provider-url: "http://localhost:11434"  # Default Ollama address
